---
layout: article
permalink: /posts/2016-05-04-harvesting-twitter-streams2
title: "Another Try at Harvesting the Twitter Streaming API to WARC files"
author: justin_littman 
excerpt: "We'e abandoning record segmentation for harvesting the Twitter Streaming API to WARC files and trying a new approach."
---

In ["Harvesting the Twitter Streaming API to WARC files"](http://gwu-libraries.github.io/sfm-ui/posts/2015-12-15-harvesting-twitter-streams), I described an approach for recording the [Twitter Streaming API](https://dev.twitter.com/streaming/overview) in WARC files using [record segmentation](http://iipc.github.io/warc-specifications/specifications/warc-format/warc-1.0/#record-segmentation).  The motivation for using record segmentation was that it allowed splitting up a single call to the API — a call that might have a very long duration and involve a large amount of data — into multiple WARC records spread across multiple WARC files.We just abandoned that approach.  Here’s why:
* There is no support for it in the web archiving toolset.  This required that we customize [warcprox](https://github.com/internetarchive/warcprox) (for capture) and the [warc python library](https://github.com/internetarchive/warc) (for reading the WARCs).  This conflicted with our goal of writing less code by borrowing existing tools from web archiving.* Playback was bothering me.  Part of our [technical approach for aligning social media archiving with web archiving](https://docs.google.com/presentation/d/1vUylMGeswqXLWwCyLVELdp60a5SIHMI2Navlk-lR9NI/pub?start=false&loop=false&delayms=3000&slide=id.g10eeeeeb04_0_142) is to load WARC files containing social media data into a wayback machine for the purpose of playback.  However, a monster HTTP response seemed an ill fit and likely to require extensive customization of some wayback implementation.
* Exports seemed potentially problematic as well.  Exporting required reconstructing and reading through monster HTTP responses.  This was particularly expensive for exports that were limited to the tweets within a time period.* It has become increasingly clear that data collected from the Twitter Streaming API MUST be considered a sample.  Some of the existing reasons for this are [rate limits in the Twitter Streaming API](https://dev.twitter.com/streaming/reference/post/statuses/filter), inevitable network hiccups or similar operational ailments that will interrupt the stream, and the simple fact that the Twitter Streaming API is a “black box” whose exact operation is unknown (well, to us anyway).  If the data collected must be considered a sample, then small interruptions in the harvest should be acceptable as long as they don’t introduce any sort of a sampling bias.  Researchers requiring a complete dataset will probably want to purchase it from a data reseller like [Gnip](https://gnip.com/sources/twitter/).Given this, we’re trying a new approach:  Harvest from the Twitter Streaming API for 30 minutes at a time.  At the end of the 30 minutes, close the stream and start a new one.  Each 30 minute segment is recorded in a single WARC response record in a single WARC file.  The interruption in collecting is only a handful of seconds.Twitter [warns against connection churn](https://dev.twitter.com/streaming/overview/connecting):  "Clients which break a connection and then reconnect frequently (to change query parameters, for example) run the risk of being rate limited."  However, we’re hoping that 30 minutes between reconnects is reasonable.The upside of this new approach is that each WARC response record is a more manageable size that should play well with existing web archiving tools and be more export friendly.  Oh yeah – and I get to throw away a ton of code.
